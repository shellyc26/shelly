# The R team tries t-tests and its alternatives

## Achievements to unlock

Leslie is ready to move on to her next inferential statistical test. Kiara explains that today they will be working on t-tests, which are used to compare two means to see if they are different from another. The test is often used to compare the mean of some variable across two groups. For example, it might be used to compare the mean income of college graduates and people who have not graduated from college. A t-test can also be used like the goodness-of-fit chi-squared was to compare the mean from one group to a population mean. For example, the t-test could be used to see if the mean age in a sample is the same as the mean age in the population that the sample was taken from.

Leslie remembers that the t-statistic has come up before when they were looking at the confidence intervals; it was used in place of $z$ when sample sizes are small. Kiara explains that the t-statistic is also used as the test statistic for a t-test. A t-test is used to compare the means of two groups to see whether they are different. Kiara explains that there are several versions of the t-test to use in different situations. Today they will discuss the **one-sample t-test**, **independent samples t-test**, and **dependent samples t-test**. The three tests all compare two means, but each test is used in a different situation, similar to how the goodness-of-fit chi-squared and the chi-squared test of independence were used in two different situations. 

Like with the chi-squared test, the NHST process will be used to organize the statistical testing. Also like with chi-squared, they will check the assumptions of each t-test and discuss alternate analysis methods when the assumptions are not met. 

Kiara puts together a list of achievements for the day:

* Achievement 1: Understanding the relationship between one categorical variable and one continuous variable using bar graphs, frequencies, and percents 
* Achievement 2: Comparing a sample mean to a population mean with a one sample t-test
* Achievement 3: Comparing two sample means with an independent samples t-test
* Achievement 4: Comparing two related sample means with a dependent samples t-test 
* Achievement 5: Computing and interpreting an effect size for significant t-tests
* Achievement 6: Examining and checking the underlying assumptions for using the t-test 
* Achievement 7: Identifying and using alternate tests for when t-test assumptions are not met

Follow Nancy, Kiara, and Leslie through the examples and exercises to test some relationships.

## The vexing vitals problem

At the end of their chi-squared day, Nancy had mentioned to Leslie that the t-test has something to do with beer. Leslie brings this up and, after a few minutes of discussion about their favorite beverages, they veer into talking about nutrition and health. Leslie describes a recent conversation with her parents where they were concerned about the health implications of high blood pressure after her dad had a recent physical exam. Leslie, Nancy, and Kiara all think they have normal blood pressure but are not sure because they have not paid close attention. Leslie says the conversation with her parents, who do not seem to be getting any younger, made her realize that she not have a good understanding of the relationship of blood pressure (or other vital signs if she is being honest) to the health behaviors she and some of her friends participate in like drinking alcohol, marijuana use, smoking, or physical activity. 

The R team decides this could be a good thing to know and they start by reading about the two blood pressure measures, systolic and diastolic. They find that systolic blood pressure is measured in millimeters of mercury or mmHG and ranges from 74 to 238, while diastolic blood pressure is also measured in mmHG and ranges from 0 to 120. Kiara does some searching and finds that the National Health and Nutrition Examination Survey (NHANES) conducted regularly by the the Centers for Disease Control and Prevention (CDC) collects blood pressure from participants. Before they read more, they go to the CDC website and explore the NHANES data and codebook for 2015-2016 [@NHANES20152016codebook]. 

To get started, Kiara downloads and saves the 2015 - 2016 NHANES data set in a *csv* file so they have a local copy that is easy to open. She wants to make sure the Leslie knows about the <span style="font-family:Lucida Console, monospace;font-weight:bold">RNHANES</span> package she used for the download that allows access to most NHANES data in R directly from the CDC website with a single command (see Box \@ref(#ch6kiara)). Follow along with the example by downloading the **nhanes_2015-2016_ch6.csv** data from edge.sagepub.com/harris1e or using the more reproducible method in Box \@ref(#ch6kiara).

```{r}
# import nhanes 2015-2016
nhanes.2016 <- read.csv("data/nhanes_2015-2016_ch6.csv")
```

Nancy notices a lull in activity and jumps in to write the code for the first graph. Because the two blood pressure measures are measured along a continuum, she considers each of the graphs from Section \@ref(achievement 2:-graphs-for-a-single-continuous-variable) that were appropriate for continuous variables and chooses a **histogram** so that they can easily see the shape of the distribution. She looks up the name of the blood pressure variables in the codebook [@NHANES20152016codebook] and finds that systolic blood pressure is variable BPXSY1. 

```{r fig.cap="Distribution of systolic blood pressure in mmHg for 2015-2016 NHANES participants."}
# open tidyverse for graphing with ggplot2
library(tidyverse)

# graph systolic blood pressure variable BPXSY1
sbp.histo <- nhanes.2016 %>% 
  ggplot(aes(x = BPXSY1)) +
  geom_histogram(fill = "#7463AC", color = "white") +
  theme_minimal() + 
  labs(x = "Systolic blood pressure (mmHg)",
       y = "NHANES participants") 
sbp.histo  
```

Leslie suggests that it looks like most people are between 100 and about 150. She finds that the CDC defines [@MeasuringBloodPressurecdcgov] normal systolic blood pressure as below 120mmHg, at-risk between 120-139, and high as 140 and above. Viewing these ranges in the histogram might be useful, so she searches for how to fill ranges with different colors and creates a histogram with purple representing normal systolic blood pressure and gray representing at-risk or high systolic blood pressure:

```{r fig.cap="Distribution of systolic blood pressure in mmHg for 2015-2016 NHANES participants."}
# graph systolic bp BPXSY1
sbp.histo <- nhanes.2016 %>% 
  ggplot(aes(x = BPXSY1, fill = BPXSY1 > 120)) +
  geom_histogram(color = "white") +
  theme_minimal() + 
  scale_fill_manual(values = c("#7463AC","gray"),
                    labels=c("Normal range", "At risk or high"),
                    name = "Blood pressure") +
  labs(x = "Systolic blood pressure (mmHg)",
       y = "NHANES participants") 
sbp.histo  
```

```{r fig.cap="Distribution of diastolic blood pressure in mmHg for 2015-2016 NHANES participants."}
# graph diastolic bp BPXDI1
nhanes.2016 %>%
  ggplot(aes(x = BPXDI1, fill = BPXDI1 > 80)) + 
  geom_histogram(color="white") + 
  theme_minimal() + 
  scale_fill_manual(values = c("#7463AC","gray"),
                    labels=c("Normal range", "At risk or high"),
                    name = "Blood pressure") + 
  labs(x="Diastolic blood pressure (mmHg)",
       y="NHANES participants")
```

It appears that more people are within the normal range for diastolic blood pressure than are in the normal range for systolic blood pressure. Looking at these two distributions, Leslie thinks that the mean systolic blood pressure in the sample is likely higher than the 120 threshold for healthy. Kiara agrees and says that one of the t-tests they are going to talk about today will actually help to test this hypothesis.

### Achievement 1: Using graphics and descriptive statistics to make a prediction

Based on observing the histograms, Leslie has predicted that the mean systolic blood pressure in the sample is higher than 120. In addition to the histogram, she can check this with the mean and standard deviation:

```{r}
# mean and sd of systolic blood pressure
nhanes.2016 %>% 
  drop_na(BPXSY1) %>%
  summarize(m.sbp = mean(BPXSY1),
            sd.sbp = sd(BPXSY1))
```

Leslie finds that the observed mean of 120.5 is just slightly higher than the threshold of 120. While it does not seem like a big difference, she looks at the histogram again and wonders if the .5 difference is enough different from 120 to be statistically significantly different. 

```{r echo = FALSE, fig.cap="Distribution of systolic blood pressure in mmHg for 2015-2016 NHANES participants."}
sbp.histo 
```

### Unlock achievement 1: Check your understanding

Which graphs could be used to determine if the distribution of a variable is normal (check all that apply):

* bar graph 
* histogram 
* scatterplot 
* box plot 
* line graph 

## Achievement 2: One sample t-test

Kiara explains that comparing the mean in the NHANES data to a hypothesized value like 120 can be done with a **one sample t-test**. The _one sample t-test_ compares a sample mean to a hypothesized or population mean. It is one of three types of t-tests:

* **one-sample t-test**: compares a mean to a population or hypothesized value  
* **independent samples t-test**: compares the means of two unrelated groups 
* **paired samples t-test**: compares the means of two related groups 

Leslie is ready to go, she looks at her notes from Section \@ref(nhst) **chi-squared** day to remind herself of the NHST process:

#. Write the null and alternate hypotheses 
#. Compute the test statistic 
#. Calculate the probability that your test statistic is at least as big as it is if there is no relationship (i.e., the null is true) 
#. If the probability that the null is true is very small, usually less than 5%, reject the null hypothesis 
#. If the probability that the null is true is not small, usually 5% or greater, retain the null hypothesis 

### NHST Step 1: Write the null and alternate hypothesis

The first step is to write the null hypothesis for statistical testing. Kiara reminds Leslie that the null hypothesis is usually a statement that claims there is **no difference** or **no relationship** between things. In this case, the null hypothesis is stating that the mean systolic blood pressure of adults in the US is 120. The NHANES data are a sample drawn from the US population, so the NHANES data should have the same characteristics as the population from which it was drawn.  In this case, the null hypothesis should state that the mean in the population (that the sample came from) is equal to the hypothesized value of 120. Leslie writes the following:

H0: There is no difference between mean systolic blood pressure in the US and the cutoff for normal blood pressure, 120 mmHG.

HA: There is no difference between the mean systolic blood pressure in the US and the cutoff for normal blood pressure, 120 mmHG.

Kiara thinks those sound great and they move on to step 2.

### NHST Step 2: Compute the test statistic 

The one-sample t-test uses the t-statistic (sort of like a z-statistic) as the test statistic:

$$
\begin{equation}
t=\frac{\bar{x}-\mu}{\frac{s}{\sqrt{n}}}
 (\#eq:onesampt)
\end{equation}
$$

In the formula for $t$, the $\bar{x}$ represents the mean of the variable to be tested, $\mu$ is the population mean or hypothesized value, $s$ is the sample standard deviation, and $n$ is the sample size. Leslie remembers the $z$ statistic calculations and notices how similar this is. The difference is that the denominator is the **standard error** rather than the **standard deviation**. Kiara reminds her that the **standard error** approximates the **standard deviation of the sampling distribution**. So, $z$ shows how many sample standard deviations some value is away from the mean, while $t$ shows how mand standard errors (or population standard deviations) the hypothesized value is away from the mean.

Nancy notices that they do not currently have the value of $n$ and she jumps on the laptop to add it. Leslie is confused, why would they not just us the $n$ shown in the Environment pane? Nancy tells her to wait a second for the code and then see if she can figure it out:

```{r}
# mean and sd of systolic blood pressure
nhanes.2016 %>% 
  drop_na(BPXSY1) %>%
  summarize(m.sbp = mean(BPXSY1),
            sd.sbp = sd(BPXSY1),
            n.spb = n())
```

Leslie notices that the value of $n$ in the output is lower than the 9,544 observations shown in the Environment pane. Nancy explains that `drop_na(BPXSY1)` has removed all the people from the sample who were missing data on the BPXSY1 command, which it looks like was more than 2,000 people. Leslie adds the values into the t-statistic formula and calculates $t$:


$$
\begin{equation}
t=\frac{120.5394-120}{\frac{18.61692}{\sqrt{7145}}}=2.45
 (\#eq:onesampt2)
\end{equation}
$$

With a $t$ of 2.45, the sample mean of 120.5394 is 2.45 standard errors above the hypothesized value of 120.

After she computes the $t$-statistic Kiara mentions that there are ways to do these calculations in R. In base R, the `t.test()` command is useful for getting the $t$ during a **one-sample t-test**. The command takes the name of the variable and the hypothesized or population value ($\mu$) to compare it to, like this: 

```{r}
# comparing mean of BPXSY1 to 120
t.test(nhanes.2016$BPXSY1, mu = 120)
```

The output contains a lot of information. The first row confirms the variable examined by the `t.test()` command. This appears correct. The second row starts with **t = 2.4491**, which is the same as the hand calculated value. The next part of the output is df = 7144. Kiara explains that, in this context, the degrees of freedom (df) are not computed using rows and columns like the chi-squared degrees of freedom. In this case, the df value is computed by subtracting 1 from the sample size. So, a df = 7144 would indicate the sample size is 7145. This is consistent with the $n$ of 7,145 from `summarize()`. The next number in the `t.test()` output is the p-value of .01435, which leads Leslie to step 3 of NHST. 

##### NHST Step 3: Compute the probability for the test statistic (p-value) 

The t-statistic is 2.45. Like the chi-squared statistic, the t-statistic has a distribution made up of all the possible values of t and how probable each value is to occur. In the case of t, the distribution looks similar to a normal distribution. Nancy looks up from her phone and sees an opportunity to code. She plots a t-distribution with 7,144 degrees of freedom:

```{r echo=FALSE, fig.cap="T distribution with df = 7,144."}
dat<-with(density(rt(100000, 7144)),data.frame(x,y))
ggplot(data = dat, mapping = aes(x = x, y = y)) +
    geom_line()+
  theme_minimal() +
  xlab("t statistic") + ylab("Probability")
```

Kiara suggests shading in the t-statistic of 2.45 and higher in the t-distribution graph like they have been doing with other statistical tests. Nancy adds the shading to represent the probability of getting a t-statistic that is 2.45 or greater **if the null hypothesis were true**. 

```{r echo=FALSE, fig.cap="T distribution (df = 7,144) shaded for values of 2.45 or higher."}
ggplot(data = dat, mapping = aes(x = x, y = y)) +
    geom_line()+
    geom_area(mapping = aes(x = ifelse(x >= 2.45 , x, 0)), fill = "#7463AC") + 
  ylim(0,.5) + xlim(-5, 5) +
  theme_minimal() +
  xlab("t statistic") + ylab("Probability")

```

The shaded section of the distribution is very small, indicating that a t-statistic of 2.45 or greater has a low probability **when the null hypothesis is true**. The output from the t-test quantifies this probability with the p-value of .014. The interpretation of this value is that there is a 1.4% probability that a t-statistic would be 2.45 or greater **if the null hypothesis were true**. That is, there is a 1.4% probability of a t-statistic equal to, or greater than, 2.45 for a sample this large from a population where the mean systolic blood pressure was 120. 

Usually a t-statistic, or any statistic, with a probability of occurring of less than 5% of the time is considered to be  **statistically significant**. For the $t$ distribution, the cutoff for values that would be occur less than 5% of the time are shown by the shaded areas. The shaded areas together make up the **critical region** or **rejection region** for the null hypothesis since the values of $t$ in these shaded areas happen so infrequently when the null hypothesis is true that is seems more likely that it should be rejected. The t-statistics falling into this critical region therefore suggest a **statistically significant** result.


```{r echo=FALSE, fig.cap="T distribution (df = 7,144) with 2.5% shaded in each tail of the distribution."}

dat<-with(density(rt(100000, 7144)),data.frame(x,y))
ggplot(data = dat, mapping = aes(x = x, y = y)) +
    geom_line() +
    geom_ribbon(data = subset(dat, x < -1.96), aes(ymax = y), ymin=0,
              fill="#7463AC") +
  geom_ribbon(data = subset(dat, x > 1.96), aes(ymax = y), ymin=0,
              fill="#7463AC") +
  ylim(0,.5) + xlim(-5, 5) +
  theme_minimal() +
  xlab("t statistic") + ylab("Probability")

```

Any value of the t-statistic that is in the shaded tales of the distribution happens less with a probability less than 5% **when the null hypothesis is true**. 

##### NHST Steps 4 & 5: Interpret the probability and write a conclusion 

In this case, the t-statistic is in the rejection region, so there is sufficient evidence to reject the null hypothesis in favor of the alternate hypothesis. Even though the difference between the mean systolic blood pressure of 120.5 and the hypothesized value of 120 is small, it is statistically significant. The probability of this sample coming from a population where the mean systolic blood pressure is actually 120 is just 1.4%. This sample is likely to be from a population with a higher mean blood pressure.

Leslie summarizes the results of this t-test: 

> The mean systolic blood pressure in a sample of 7,145 people was 120.5 (sd = 18.62). A one-sample t-test found this mean to be statistically significantly different from the hypothesized mean of 120 [t(7144) = 2.45; p = .014]. The sample likely came from a population with a mean systolic blood pressure higher than 120.

### Unlock achievement 2: Check your understanding

Interpret the results of the same one-sample t-test conducted after limiting the NHANES sample to people 65 years old or older:

```{r}
# subset data frame to 65+ years old
nhanes.2016.65plus <- nhanes.2016 %>%
  filter(RIDAGEYR >= 65) 

# comparing mean of BPXSY1 to 120
t.test(x = nhanes.2016.65plus$BPXSY1, mu = 120)
```

## Achievement 3: Independent samples t-test

Leslie thinks this is cool for checking to see how well your sample represents a population, but asks if the t-test can be used for anything else. Kiara explains that there are two additional types of t-test. Instead of comparing one mean to a hypothesized or population mean, the **independent samples t-test** compares the means of two groups to each other. For example, the NHANES data set includes sex measured in two categories: males and females. We might be interested in whether the mean systolic blood pressure is the same for males and females in the population. That is, do males and females in the sample come from populations where they have the same mean systolic blood pressure? 

Kiara suggests the independent samples t-test can be used to find out if this is the case. Before conducting NHST, Leslie starts with some descriptive and visual EDA. She notes that they now have the blood pressure variable measured on a continuum and treated as continuous, and the sex variable that is categorical. Since they are comparing mean blood pressure across groups, she decides to start with some group means:

```{r}
# compare means of BPXSY1 across groups
# sex variable is RIAGENDR
nhanes.2016 %>% 
  drop_na(BPXSY1) %>%
  group_by(RIAGENDR) %>%
  summarize(m.sbp = mean(BPXSY1))

```

Well, it certainly looks like there may be a difference, but it is unclear who has higher or lower blood pressure since the categories of sex are not labeled clearly. Nancy slides the laptop over and does some cleaning for both the variables. She looks in the codebook to find out how the RIAGENDR variable is coded. She finds the entry in the demographics codebook:

```{r echo = FALSE, fig.cap="NHANES 2015-2016 demographics codebook: gender variable."}
knitr::include_graphics("graphics/chap6-gender-nhanes-coding.JPG")
```

It looks like 1 is Male and 2 is Female. Nancy adds the labels and renames the variable:

```{r}
# add labels to sex and rename variables
nhanes.2016.cleaned <- nhanes.2016 %>%
  mutate(RIAGENDR, RIAGENDR = recode_factor(.x = RIAGENDR, 
                                            `1` = 'Male',
                                            `2` = 'Female')) %>%
  rename(sex = RIAGENDR) %>%
  rename(systolic = BPXSY1)


```

Once the recoding is done, Nancy updates the means calculations with the new data frame name and variable names:

```{r}
# compare means of systolic by sex
nhanes.2016.cleaned %>% 
  drop_na(systolic) %>%
  group_by(sex) %>%
  summarize(m.sbp = mean(systolic))
```

It appears that males have a higher mean systolic blood pressure than females. Leslie thinks a graph might help give a little more perspective, so she looks back at Section \@ref(catcont) and decides on a density plot:

```{r}
box.sex.bp <- nhanes.2016.cleaned %>%
  ggplot(aes(x = systolic, 
             fill = sex)) +
  geom_density(alpha = .8) +
  theme_minimal() +
  labs(x = "Systolic blood pressure") +
  scale_fill_manual(values = c('gray', '#7463AC'),
                    name = "Sex") 
box.sex.bp 
```

The density plot shows distributions that are a little right skewed, with the distribution for males shifted to the right of the distribution for females showing higher values overall but not by a lot. Now that she knows what to look for, Leslie is ready for the independent samples t-test. She uses the NHST to organize her work:

#### NHST Step 1: Write the null and alternate hypotheses

H0: There is no difference in mean systolic blood pressure for males and females in the US population 

HA: There is a difference in mean systolic blood pressure for males and females in the US population 

#### NHST Step 2: Compute the test statistic 

The test statistic for the independent samples t-test is a little more complicated to calculate since it now includes the means from both the groups in the numerator and the standard errors from the groups in the denominator. In the formula, $\bar{x_1}$ is the mean of one group and $\bar{x_2}$ is the mean of the other group; the difference between the means makes up the numerator. So, the larger the difference between the group means, the larger the numerator will be and the larger the $t$-statistic will be!

The denominator includes the standard deviations for the first group, $s_1$, and for the second group, $s_2$ along with the sample sizes for each group, $n_1$ and $n_2$. 

$$
\begin{equation}
t=\frac{\bar{x_1}-\bar{x_2}}{\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}}
 (\#eq:indept)
\end{equation}
$$

Nancy adds the standard deviation and $n$ to the summarize statistics:

```{r}
# compare means of systolic by sex
nhanes.2016.cleaned %>% 
  drop_na(systolic) %>%
  group_by(sex) %>%
  summarize(m.sbp = mean(systolic),
            sd.sbp = sd(systolic),
            n = n())
```

Leslie substitutes them into the t-statistic formula:

$$
\begin{equation}
t=\frac{122.1767 - 118.9690}{\sqrt{\frac{18.14654^2}{3498}+\frac{18.92703^2}{3647}}}=7.31
 (\#eq:indept)
\end{equation}
$$

After watching Leslie substitute in the values and do the math, Nancy types a line of code:

```{r}
# compare systolic blood pressure for males and females
t.test(formula = nhanes.2016.cleaned$systolic ~ nhanes.2016.cleaned$sex)
```

This time, instead of the first argument being `x =`, Nancy has typed `formula =`. In R, a formula typically (but not always) has a single variable on the left, followed by a `~` (i.e., tilde), followed by one or more objects that somehow predict or explain whatever was on the left hand side. In a lot of statistical tests, the object on the Left-Hand-Side (LHS) of the formula is the **outcome** or **dependent variable** while the object(s) on the Right-Hand-Side (RHS) of the formula are the **predictors** or **independent variables**. In this case, systolic blood pressure is the **outcome** being explained by **sex**. 

The hand calculation and `t.test()` output both show a t-statistic of 7.31, which is consistent with what Leslie calculated. The degrees of freedom are 7,143, which is the sample size of 7,145 minus two because there are two groups. In the case of the **independent samples t-test**, the degrees of freedom are computed as n - k, where n is the sample size and k is the number of groups.

Leslie takes a look at the help documentation for `t.test()` to see what else it has to offer. She notices that the help documentation calls the t-test, "Student's t-Test." (Figure \@ref(fig:studentt))

```{r echo = FALSE, fig.cap="NHANES 2015-2016 demographics codebook: gender variable."}
knitr::include_graphics("graphics/chap6-student-ttest-doc.JPG")
```

#### NHST Step 3: Compute the probability for the test statistic (p-value) 

The p-value in this case is shown in _scientific notation_, so Leslie converts it: p = .0000000000002886. Kiara suggests using p < .05 instead since the longer number is more difficult to read and takes up a lot of space. The value of this t-statistic happens with a probability of much less than 5%. 

#### NHST Steps 4 & 5: Interpret the probability and write a conclusion 

In this case, the t-statistic is in the rejection region, so there is sufficient evidence to reject the null hypothesis in favor of the alternate hypothesis. Even though the difference between the mean systolic blood pressure for males and females is small, it is statistically significant. The probability of the sample coming from a population where the means for males and females are equal is very low, it would happen about 1.4% of the time. This sample is likely to be from a population where males and females have different mean systolic blood pressure.

Kiara summarizes the interpretation: There is a statistically significant difference [t(7143) = 7.31; p < .05] between the mean systolic blood pressure for males (m = 122.2) and females (m = 119.0). The sample is from the US population indicating that males in the US likely have a higher mean systolic blood pressure than females in the US.

### Unlock achievement 3: Check your understanding

Conduct the same independent samples t-test on the sample created in the previous section of people 65+ years old. Interpret your results.

## Achievement 4: Dependent samples t-test

Nancy mentions that sometimes the means to compare will be related. This usually happens in one of two ways, either the same people are measured twice or people are siblings or spouses or co-workers or have some other type of relationship. Nancy explains that it may seem strange to measure the same people twice, but often people will be measured before and after some sort of intervention and the measures are compared to see if they changed. 

The dependent samples t-test works a little differently from the independent samples t-test. In this case, it is the mean of the differences between the first and second measures. So, for example, if Nancy's systolic blood pressure was measured to be 110 on the first measure and 112 on the second measure, the difference between the two is 2. If Kiara were measured as having 115 first and 110 second, the difference between the two measures is -5. The numerator for the paired t-test would take the mean of those differences and subtract 0 since this is the hypothesized mean difference when two measures are the same. The denominator also takes the differences into consideration:

$$
\begin{equation}
t=\frac{\bar{d}-0}{\sqrt{\frac{s_d^2}{n}}}
 (\#eq:dept)
\end{equation}
$$

In the case of the systolic blood pressure measurement for NHANES, the measure is taken up to four times for each person to ensure that it is accurate. In this case, Kiara notes, these measures should be the same or very similar, so if they are measured accurately there would not be a significant difference between blood pressure measure 1 and blood pressure measure 2. She suggests that Leslie find the difference between the two measures and use descriptive and visual EDA to examine the data.

Leslie starts by making a new variable:

```{r eval = FALSE, fig.cap="Difference between measure 1 and 2 for systolic blood pressure (mmHg)."}
# difference between measure 1 and 2 for systolic BP
nhanes.2016.cleaned$diff.sbp <- nhanes.2016.cleaned$systolic - nhanes.2016.cleaned$BPXSY2

# mean of the differences
mean(nhanes.2016.cleaned$diff.sbp, na.rm = TRUE)

# histogram of the differences
ggplot(nhanes.2016.cleaned, aes(diff.sbp)) + 
  geom_histogram(fill = "#88398a", color="grey") + 
  theme_minimal() + 
  labs(x="Difference between measure 1 and 2",
       y="NHANES participants")

```

The distribution of differences looks close to normal and the center is near 0, but not exactly 0. The mean difference is .54. If measures 1 and 2 were exactly the same for each person, the mean difference would be 0. Using the NHST process, Leslie tests to see if the .54 mean difference is significantly different from the 0 she would expect to see if the measures were perfectly consistent:

#### NHST for the dependent samples t-test

##### NHST Step 1: Write the null and alternate hypotheses

H0: There is no difference between measures 1 and 2 for systolic blood pressure  

HA: There is a difference between measures 1 and 2 for systolic blood pressure 

##### NHST Step 2: Compute the test statistic

Leslie substitutes the mean, variance, and sample size of the difference between the measures into the formula for the paired t-test statistic. First she needs the variance and sample size:

```{r eval = FALSE}
# var and sample size of the difference variable
var(nhanes.2016.cleaned$diff.sbp, na.rm = TRUE)
summary(nhanes.2016$diff.sbp)

```

Note that the sample size is smaller this time, 2,443 people had missing data for one or both of the systolic blood pressure measures. With 9,544 people in the full data set, this means that 7,101 people had data for both measures available. 

\[
t=\frac{.54-0}{\sqrt{\frac{23.88}{7101}}}=9.38
\]

Kiara asks Leslie what she expects to find with a t-statistic this large. In light of the significant tests for the one-sample and independent samples t-tests, Leslie thinks this is likely to be a _statistically significant_ result as well. Kiara suggests using the `t.test()` command again, but this time with the `paired = TRUE` option since the default for the command is an independent samples test. Leslie tries it: 

```{r eval = FALSE}
# paired t-test for systolic measures 1 and 2
t.test(x = nhanes.2016.cleaned$systolic, 
       y = nhanes.2016.cleaned$BPXSY2, 
       paired = TRUE)
```

##### NHST Step 3: Compute the probability for the test statistic (p-value) 

Once again, the p-value is shown in scientific notation, with an actual value of .00000000000000022. Just as Leslie suspected, the p-value is well below .05 indicating that the probability of a mean difference of .54 is much lower than 5%.

##### NHST Steps 4 & 5: Interpret the probability and write a conclusion 

Once again, the t-statistic has a low probability and is in the rejection region, so there is sufficient evidence to reject the null hypothesis in favor of the alternate hypothesis. Even though the mean difference between the first and second measures is small, it is statistically significant. The probability of the sample coming from a population where the measures are equal is very low. This sample is likely to be from a population where systolic blood pressure is not measured consistently over time.

Kiara summarizes the interpretation: The mean difference between systolic blood pressure measures is statistically significantly different from zero [t(7100) = 9.38; p < .05]. The positive difference of .54 indicates that systolic blood pressure is significantly higher for the first measure compared to the second measure.

Note that the mean difference is quite small and does not seem to be reason to worry. While it is not zero, as it would be if the measures were completely consistent, it is .54 on average, which is not a large clinical difference. This example and the small but significant differences for the one-sample and independent samples t-tests demonstrate that results can be statistically significant but not clinically meaningful. 

A review of the formulas for the three t-statistics shows one possible reason why the results of these three t-tests were all significant. The denominator of all three tests includes the sample size, which was large. Large sample sizes in these denominators resulted in small denominators which, in turn, result in large t-statistics.

### Unlocking achievement 4: Check your understanding

Match each type of t-test to its description:

t-tests: 

* One-sample  
* Dependent samples 
* Independent samples 

descriptions: 

* Compares means of two unrelated groups 
* Compares a single mean to a population or hypothesized value 
* Compares two related means 

### Achievement 5: Introducing assumptions

So far the only requirements for the chi-squared and t-tests have been having the right variable types. Chi-squared is used for examining the relationship between two categorical variables while t-tests are used for comparing means of continuous variables between two groups. 

In addition to variable type requirements, Nancy explains, there are other qualities of the data necessary for the tests to be accurate. These qualities are called _assumptions_ and are part of the foundation for each test.

The first assumption for the t-test is that the data are normally distributed. For the one-sample t-test, the single variable being examined should be normally distributed. For the independent samples t-test and the paired samples t-test the data within each of the two groups should be normally distributed. There are several ways to assess normality. Visually, a histogram or a QQ-plot is useful for identifying normal and non-normal data distribution. Statistically, a Shapiro-Wilk test can be used. 

#### Testing normality 

For the one-sample t-test comparing systolic blood pressure to a hypothesized population mean of 120, the histogram to determine whether a t-test was appropriate would look like this:

```{r  eval = FALSE, fig.cap="Distribution of systolic blood pressure in mmHg\nfor 2015-2016 NHANES participants"}
# graph systolic bp BPXSY1
ggplot(nhanes.2016.cleaned, aes(systolic)) + 
  geom_histogram(fill = "#88398a", col = "grey") + 
  theme_minimal() + 
  labs(x="Systolic blood pressure (mmHg)",
       y="NHANES participants") 
```

The histogram does not suggest a normal distribution. The data appear right skewed. Another way to visually check normality is with a QQ-plot, or quantile-quantile plot. This plot is made up of points below which a certain percentage of the observations fall. On the x-axis are normally distributed values with a mean of 0 and a standard deviation of 1. On the y-axis are the observations from the data. If the data are normally distributed, the values will form a diagonal line through the graph. 

```{r  eval = FALSE, echo = FALSE, fig.cap="Distribution of systolic blood pressure in mmHg for 2015-2016 NHANES participants"}

# graph systolic bp BPXSY1
ggplot(nhanes.2016.cleaned, aes(sample = systolic)) + 
  stat_qq(color = "#88398a") + 
  geom_abline(intercept = mean(nhanes.2016$systolic, na.rm = TRUE), 
              slope = sd(nhanes.2016$systolic, na.rm = TRUE)) +
  theme_minimal() + 
  labs(x = "Theoretical normal distribution",
       y = "Observed values")
  
```

Consistent with the right-skewed histogram, the higher observed values at the top of the graph are further from the line representing normality. In this case, the visual evidence is enough to state that this assumption is not met. If the graphs showed the data were closer to normal, a statistical test can contribute to confirming whether the data are or are not normally distributed. 

The Shapiro-Wilk test for normality has the following null and alternate hypotheses:

H0: The data are normally distributed.

HA: The data are not normally distributed.

In R, the test is conducted using the `shapiro.test()` command. Because the Shapiro-Wilk test is likely to always be significant in very large samples, the command only works when there are 5,000 observations or fewer. Had the sample size been smaller, this command would test the null hypothesis:

```{r eval = FALSE}
# statistical test of normality for systolic variable
shapiro.test(x = nhanes.2016.cleaned$systolic)
```

To demonstrate how the test works, Kiara suggests taking a sample from the NHANES data and testing the sample. The `sample()` command is useful for taking a random sample from a data frame in R. Nancy suggests 1,000 people as the sample size. Kiara writes a small amount of code, explaining that the `sample()` command selects the number of people specified and then the sub-setting format with square brackets can be used to take the subset of these people:

```{r eval = FALSE}
# set a starting value
set.seed(42)

# sample 1000 cases from nhanes.2016 and name nhanes.2016sample
nhanes.2016sample <- nhanes.2016.cleaned[sample(1:nrow(nhanes.2016.cleaned), 1000), ]
```

To understand the distribution of this smaller data set, start by graphing it:

```{r eval = FALSE, fig.cap="Distribution of systolic blood pressure in mmHg for 1,000 2015-2016 NHANES participants."}

#graph systolic bp BPXSY1
ggplot(nhanes.2016sample, aes(systolic)) + 
  geom_histogram(fill = "#88398a", col = "grey") + 
  theme_minimal() + 
  labs(x="Systolic blood pressure (mmHg)",
       y="NHANES participants")
  
```


The smaller sample appears even more right skewed than the full NHANES data set, so it is unlikely to be normal by the Shapiro-Wilk test:

```{r eval = FALSE}
# statistical test of normality for systolic variable
shapiro.test(x = nhanes.2016sample$systolic)
```

The result of the test is that the probability that the data are normally distributed is .000000000000015. This is a tiny probability and typically would result in rejecting the null hypothesis that the data are normally distributed. Instead, the results of this test favor the alternate hypothesis. Leslie tries her hand at an interpretation: A Shapiro-Wilk test for normality rejected the null hypothesis (W = .95; p < .05) and found the systolic blood pressure variable to be non-normal. The variable fails the assumption of a normal distribution. 

Normality is checked for _each group_ for the independent samples t-test and paired samples t-test. Kiara explains that histograms can be shown together on the same x and y axes using different colors, or on separate plots. Leslie thinks plotting them separately would be best to be able to see the shape clearly for each group. Kiara copies the `ggplot()` code from earlier and adds a `facet_grid()` option with the facets identified by the sex variable. 

```{r  eval = FALSE, fig.cap="Distribution of systolic blood pressure in mmHg for 1,000 2015-2016 NHANES participants"}
#graph systolic bp by sex
ggplot(nhanes.2016sample, aes(systolic)) + 
  geom_histogram(fill = "#88398a", col = "grey") + 
  facet_grid(sex ~ .) + 
  theme_minimal() + 
  ylim(c(0,60)) + 
  labs(x="Systolic blood pressure (mmHg)",
       y="NHANES participants")
  
```

The two separate groups look right skewed, like the overall distribution. A qq-plot with facets might be able to confirm:

```{r  eval = FALSE, fig.cap = "Distribution of systolic blood pressure in mmHg\nfor 2015-2016 NHANES participants."}
#graph systolic bp
ggplot(nhanes.2016.cleaned, aes(sample = systolic)) + 
  stat_qq(color = "#88398a") +
  facet_grid(sex ~ .) + 
  geom_abline(intercept = mean(nhanes.2016.cleaned$systolic, na.rm = TRUE), 
              slope = sd(nhanes.2016.cleaned$systolic, na.rm = TRUE)) +
  theme_minimal() + 
  labs(x="Theoretical normal distribution",
       y="Observed values")
```

The groups clearly _fail the assumption of normal distribution_. The Shapiro-Wilk test of normality can help to confirm this statistically for each of the two groups:

```{r eval = FALSE}
# statistical test of normality for Male systolic bp
shapiro.test(x = nhanes.2016.cleaned$systolic[nhanes.2016.cleaned$sex == "Male"])
shapiro.test(x = nhanes.2016.cleaned$systolic[nhanes.2016.cleaned$sex == "Female"])
```

With a statistically significant result, the Shapiro-Wilk test suggests that systolic blood pressure is not normally distributed among males (W = .95; p < .05) or among females (W = .94; p < .05). 

Testing normality for the paired-samples t-test is similar to the one-sample t-test, just graph or statistically test the diff.sbp variable to see if the differences between the first and second measures are normally distributed.

```{r  eval = FALSE, fig.cap="Distribution of differences in systolic blood pressure readings \nfor 2015-2016 NHANES participants."}
#graph systolic bp BPXSY1
ggplot(nhanes.2016.cleaned, aes(diff.sbp)) + 
  geom_histogram(fill = "#88398a", col = "grey") + 
  theme_minimal() + 
  labs(x="Difference between measures of systolic blood pressure (mmHg)",
       y="NHANES participants")
```

Leslie notes that this distribution looks more normal than any of the previous ones. She tries a qq-plot to see if her observation holds:

```{r  eval = FALSE, fig.cap="Distribution of differences in systolic blood pressure measures\nfor 2015-2016 NHANES participants."}

#graph systolic bp BPXSY1
ggplot(nhanes.2016.cleaned, aes(sample = diff.sbp)) + 
  stat_qq(color = "#88398a") + 
  geom_abline(intercept = mean(nhanes.2016.cleaned$diff.sbp, na.rm = TRUE), 
              slope = sd(nhanes.2016.cleaned$diff.sbp, na.rm = TRUE)) +
  theme_minimal() + 
  labs(x="Theoretical normal distribution",
       y="Observed values")
  
```

Leslie is a little disappointed that the variable does not look normally distributed in this plot, but decides to try a statistical test of the variable from the small sample data frame to check one more time:

```{r eval = FALSE}
# statistical test of normality for difference variable
shapiro.test(x = nhanes.2016sample$diff.sbp)
```

Despite the promising histogram, the qq-plot and Shapiro-Wilk test suggest that the difference variable is not normally distributed (W = .96; p < .05).

Leslie notes that _none_ of t-tests met the _normal distribution_ assumption! While failing this assumption would be enough of a reason to choose another test, Nancy explains that there is one additional assumption to test for the independent samples t-test. The second assumption is the assumption of _homogeneity of variances_ or _equal variances across groups_. So, not only do the data need to be normally distributed, but the data should be equally spread out in each group. Leslie reviews the graphs thinks this might actually be an easier assumption to meet.

Nancy mentions that the _Levene's Test_ is widely used to test the assumption of equal variances. The null hypothesis is that _the variances are equal_ while the alternate is that at least two of the variances are different. The **car** package in R has a Levene's test command that can be used to test for equal variances:

```{r eval = FALSE}
library(car)

# equal variances for systolic by sex
leveneTest(y = systolic ~ sex, data = nhanes.2016.cleaned)
```

The Levene's test finds that there is no statistical reason to reject the null hypothesis for _sex_; the variances of systolic blood pressure for men and women are not statistically significantly different (p = .06). So, the independent samples t-test meets the assumption of _homogeneity of variances_.

So, overall, none of the tests passed all assumptions. All of the tests failed the assumption of normal distribution.

Nancy summarizes the assumptions for the tests to make sure Leslie has them all:

#### one-sample t-test assumptions

* continuous variable 
* independent observations 
* normal distribution 

#### independent-samples t-test assumptions 

* continuous variable and two independent groups 
* independent observations 
* normal distribution in each group 
* equal variances for each group 

#### dependent-samples t-test assumptions 

* continuous variable and two dependent groups 
* independent observations  
* normal distribution of differences 

Leslie asks Nancy what the difference is between independent observations and independent groups. Nancy explains that independent observations is the assumption that the people in your data are not related to one another in any important way. Things that might violate this assumption are having siblings or spouses in a data set or measuring the same person multiple times. Independent groups is the assumption that two groups are not related to one another. If one group was made up of all the spouses of another group, the two groups would not be independent.

Leslie expresses some disappointment that none of the statistical tests passed the assumptions. Kiara explains that each test has a variation for when the assumptions are not met.

### Unlocking achievement 5: Check your understanding

Which independent samples t-test assumption appears to be violated by the data in the graph shown? 

```{r  eval = FALSE, echo = FALSE, fig.cap="Distribution of data violating a t-test assumption."}
ggplot(subset(nhanes.2016.cleaned[nhanes.2016.cleaned$INDFMPIR < 5, ], !is.na(sex)), aes(INDFMPIR)) + 
  geom_histogram(fill = "#88398a", col = "grey") + 
  facet_grid(sex ~ .) +
  theme_minimal() 
  
```

* continuous variable and two independent groups 
* independent observations 
* normal distribution in each group 
* equal variances for each group 

## Achievement 6: Sign test, Mann-Whitney U, and other alternative tests when assumptions are not met

Nancy introduces Leslie to the alternate tests she can use when the assumptions fail:

* one-sample t-test -> Sign test 
* paired-samples t-test -> Wilcoxon signed rank test
* independent-samples t-test -> Mann-Whitney U or Kolmogorov-Smirnoff 

### Sign test

When the data failed the assumption of normality for a one-sample t-test, the median could be examined rather than the mean, just like in descriptive statistics. The alternative to the one-sample t-test is the _Sign test_, which tests whether the _median_ of a variable is equal to some hypothesized value. In this case, Nancy suggests:

H0: The median systolic blood pressure in the US population is 120 

HA: The median systolic blood pressure in the US population is not 120

Leslie looks up the _Sign test_ and finds that it is conducted using the `SIGN.test()` command from the BSDA package. She installs the BSDA package and then loads it to conduct the test. Before conducting the test, she remembers that EDA is an important step and examines the median value of the systolic variable.

```{r eval = FALSE}

# examine median for systolic variable
median(nhanes.2016.cleaned$systolic, na.rm = TRUE)

```

The median systolic blood pressure is 118. This is close but a little lower than the 120 hypothesized to be the median value. Nancy reminds Leslie and Kiara that the median value is exactly in the middle with the same number of observations above as below it. Leslie opens the BSDA library and tests the median of 118 against the hypothesized median of 120. The `md =` option in the `SIGN.test()` indicates the hypothesized value to test.


```{r eval = FALSE}
# sign test in BSDA package
library(BSDA)

# conduct median test
SIGN.test(nhanes.2016.cleaned$systolic, md = 120)

```

Kiara interprets the results: The median systolic blood pressure for NHANES participants was 118. A Sign-test comparing the median to a hypothesized median of 120 had a statistically significant (s = 3004; p < .05) result. We reject the null hypothesis that the US population median systolic blood pressure is 120 in favor of the alternate that the median is not 120.

Nancy notes that the output includes a 95% confidence interval of 116 to 118. In this case, she says, the confidence interval suggests that this sample likely came from a population where the median systolic blood pressure was between 116-118. So, the median in the sample is 118 and the median in the population the sample came from is likely between 116-118.

Leslie asks about the list of confidence intervals at the end of the output. Kiara and Nancy think those can be ignored for now, but may come back later.

### Wilcoxon signed-rank test

This test is an alternative to the _paired-samples t-test_ when the continuous variable is not normally distributed. The Wilcoxon test determines if two related samples come from populations with the same distribution. That is, instead of comparing means, the test compares the overall distributions of the two groups. 

Nancy thinks it is important to consider way the test compares distributions, since many of the alternate tests use a similar process. Nancy explains the steps used to compute the test statistic:

Step 1: Find the differences between the two paired measures (measure 1 - measure 2)

Step 2: Put the absolute values of the differences in order from smallest to largest and give each one a rank

Step 3: Sum the ranks for all the _positive_ differences

Step 4: Sum the ranks for the _negative_ differences 

The test statistic for this test (V) is the smaller of the _Step 3_ and _Step 4_ values. Unless a sample size is very small (n(n+1)/2 < 20), the test statistic value is used to compute a z-score:

\[
q=\frac{W - \frac{n(n+1)}{4}}{\sqrt{\frac{n(n+1)(2n+1)}{24}-\frac{(t^3-t)}{48}}}
\]

Where t is the number of ties in rank. The z-score is then translated into a corresponding p-value.  

The null and alternate hypotheses are:

H0: There is no difference in systolic blood pressure measures taken at time 1 and time 2 in the US population.

HA: There is no difference in systolic blood pressure measures taken at time 1 and time 2 in the US population.

In R, the `wilcox.test()` in the MASS package tests the null hypothesis. Kiara reminds Leslie to include the `paired = TRUE` argument in the code:

```{r eval = FALSE}
# open the MASS library
library(MASS)

# test the distribution of SBP by time period
wilcox.test(nhanes.2016.cleaned$systolic, nhanes.2016.cleaned$BPXSY2, paired = TRUE)
```

So, the smallest sum of ranks is 9,550,000. 

The resulting output is similar to the output for many of the tests. Leslie interprets it: A Wilcoxon signed-rank test distribution of systolic blood pressure at time 1 and time 2 is _not_ the same in the US population (V = 9,550,000; p < .05). 

A graph of the two distributions might demonstrate the difference:

```{r  eval = FALSE, fig.cap="Distribution of systolic blood pressure by measure 1 and 2 for 2015-2016 NHANES participants."}
library(tidyverse)

# make subset of data long to graph
vars <- c("BPXSY1","BPXSY2","SEQN")
nhanes.2016.long.sbp <- gather(nhanes.2016.cleaned[vars], measure, sbp, BPXSY1:BPXSY2, factor_key = TRUE) 

# with lines
ggplot(nhanes.2016.long.sbp, aes(x=sbp, fill=measure)) +
  geom_histogram(fill = "#88398a", col = "grey") + 
  facet_grid(measure ~ .) + 
  theme_minimal() + 
  labs(x="Systolic blood pressure (mmHg)",
       y="NHANES participants")

```

The distributions look quite similar, but have enough minor differences to reach statistical significance. In this case, the difference may be _statistically significant_ but not large enough to be concerned about the measurement strategy for NHANES.

Note that this test, the Mann-Whitney U, the K-S test, and the Kruskal-Wallis test are often interpreted as testing for _equal medians_. While none of these tests examine medians directly like a t-test examined means directly, the ordering and ranking is similar to how medians are identified, so there is some logic to this interpretation. However, if the distribution shape or spread (or both) are different, the interpretation of results with respect only to the median can be very misleading. 

For example, data can have far different shape and still have the same median value. Kiara simulates two  synthetic data sets with 125 observations and nearly equal medians. She examines the medians and conducts a Wilcoxon signed rank test:

```{r  eval = FALSE, echo = FALSE}
# data with mean of 100 and different sd
set.seed(42)
demo1 <- rnorm(n = 125, mean = 100, sd = 2)
demo2 <- rnorm(n = 125, mean = 100, sd = 10)
spread_example <- data.frame(demo1, demo2)
```

```{r eval = FALSE}
# medians
median(demo1)
median(demo2)

# test of medians
wilcox.test(x = spread_example$demo1, 
            y = spread_example$demo2, 
            paired = TRUE)
```
The medians are close at 100.16 and 99.14. The Wilcoxon test would _fail to reject_ the null hypothesis in this case (V = 4191; p = .53), resulting in the conclusion that the two measures are not different from one another. While it is true that the medians are similar, graphs of the two variables shows extremely different distributions. The first variable shows a narrow distribution with all values close together, while the second shows a wide distribution with values spread far. The _central tendency_ is similar, but the _spread_ is not. This may or may not be important to a research project, but should at least be noted with the test results.

```{r  eval = FALSE, echo = FALSE, warning = FALSE, message = FALSE, fig.cap="Two synthetic measures on one group."}

spread.long <- gather(spread_example, narrow, measure, demo1:demo2, factor_key = TRUE) 

# with lines
ggplot(spread.long, aes(x=measure)) +
  geom_histogram(fill = "#88398a", col = "grey") + 
  facet_grid(narrow ~ .) + 
  theme_minimal() + 
  labs(x="Synthetic measure",
       y="Frequency") 


```

To conclude that these measures are not different from one another seems limited at best and fairly misleading. Remember to conduct visual and descriptive analyses before (or with) inferential analyses in order to interpret your results accurately. The Wilcoxon signed rank test should be accompanied by a histogram or box plot comparing the groups to support or clarify the statistical results.

#### Mann-Whitney U test or Wilcoxon rank-sum test

This test is an alternative to the independent-samples t-test when the continuous variable is not normally distributed. This test also relaxes the variable type assumption and can be used for ordinal variables in addition to continuous variables. Similar to the _Wilcoxon_ rank-sum test, the Mann-Whitney U test puts all the values for the continuous variable in order, assigns each value a rank, and compares sums of ranks in the two groups. The formula for the test statistic is to sum all the ranks (R) for each group and subtract the sum of possible ranks for a group of that size (n(n+1)/2), resulting in a zero or positive value.

\[
U_1=R_1 - \frac{n_1(n_1+1)}{2}
\]

\[
U_2=R_2 - \frac{n_2(n_2+1)}{2}
\]

Like the Wilcoxon signed-rank test, the lower of the two U statistics is the test statistic. A z-score is then computed to translate the U into a corresponding p-value:

\[
z=\frac{U - m_U)}{s_U}
\]

Where m and s are the mean and standard deviation of U.

The null and alternate hypotheses would be: 

H0: There is no difference in systolic blood pressure for males and females in the US population.

HA: There is a difference in systolic blood pressure for males and females in the US population.

The same command used in the Wilcoxon rank-sum test can be used here with a formula (See Pull-Out Box) and without the `paired =` argument:

```{r eval = FALSE}
# test the distribution of SBP by sex
wilcox.test(nhanes.2016.cleaned$systolic ~ nhanes.2016$sex)

```

The Mann-Whitney U test comparing systolic blood pressure for males and females in the US found a _statistically significant difference_ between the two groups (W = 7,186,900; p < .05). Histograms demonstrate the differences with notably more females at the lower end of the histogram compared to males.

```{r  eval = FALSE, fig.cap="Distribution of systolic blood pressure in mmHg\nfor 2015-2016 NHANES participants."}
#graph systolic bp by sex
ggplot(nhanes.2016.cleaned, aes(systolic)) + 
  geom_histogram(fill = "#88398a", col = "grey") + 
  facet_grid(sex ~ .) + 
  theme_minimal() + 
  labs(x="Systolic blood pressure (mmHg)",
       y="NHANES participants") 
  
```

Again, the test compares ranks but it is often interpreted as comparing medians. As with the Wilcoxon signed-rank test, this can be misleading. For example, Kiara creates a synthetic data set with two groups that have nearly equal medians:

```{r  eval = FALSE, echo = FALSE}
# data with mean of 100 and different sd
set.seed(450)
group1 <- rnorm(n = 200, mean = 50, sd = 5)
group2 <- rnorm(n = 200, mean = 50, sd = 20)
groups_example <- data.frame(group1, group2)
groups.long <- gather(groups_example, group, measure, group1:group2, factor_key = TRUE) 
```

```{r eval = FALSE}
# medians
median(group1)
median(group2)

# test of medians
wilcox.test(groups.long$measure ~ groups.long$group)
```

The non-significant result would suggest that there is no difference between the groups. While this may be true for the medians, which are very close to each other, the result does not take into consideration the spread or shape of the data. When graphed, these two groups look very different despite the statistical test indicating no difference. Interpreting the results should take into consideration important differences beyond the medians.

```{r  eval = FALSE, fig.cap="Synthetic measure for two groups."}
#graph systolic bp by sex
ggplot(groups.long, aes(measure)) + 
  geom_histogram(fill = "#88398a", col = "grey") + 
  facet_grid(group ~ .) + 
  theme_minimal() + 
  labs(x="Synthetic measure",
       y="Frequency") 
  
```

#### The Kolmogorov-Smirnov (or K-S) test

The KS test is an alternative to the independent-samples t-test when the homogeneity of variances assumption is _not met_. That is, KS is used when the variances in the two groups are different whether or not the normality assumption is met. 

When variances are unequal, the larger variance has a bigger influence on the size of the t-statistic, so one group is dominating the calculations:

\[
t=\frac{\bar{x_1}-\bar{x_2}}{\sqrt{\frac{s_1^2}{N_1}+\frac{s_2^2}{N_2}}}
\]

Unlike the Wilcoxon rank-sum and Mann-Whitney U, the K-S test compares the the distributions of the groups:

H0: The distribution of systolic blood pressure for males and females is the same in the US population.

HA: The distribution of systolic blood pressure for males and females is not the same in the US population.

The `ks.test()` command is used to test the null hypothesis. 

```{r eval = FALSE, }
# test the distribution of SBP by sex
ks.test(nhanes.2016.cleaned$systolic[nhanes.2016.cleaned$sex == "Male"], nhanes.2016.cleaned$systolic[nhanes.2016.cleaned$sex == "Female"])
```

The K-S test comparing the distribution of systolic blood pressure for males and females in the US found a _statistically significant difference_ between the two groups (D = .11; p < .05). The test statistic, _D_ is the maximum distance between the empirical cumulative distribution functions (EDCF) of the two groups. The ECDF is the probability distribution when samples are taken from a sample rather than from a population. 

To examine the difference between the ECDF for Males and Females, graph the two ECDF curves:

```{r  eval = FALSE, fig.cap="ECDF of systolic blood pressure in mmHg for 2015-2016 NHANES participants."}
ggplot(nhanes.2016.cleaned, aes(systolic, color = sex)) +
  stat_ecdf(size = 1.25) + 
  theme_minimal() + 
  labs(x="Systolic blood pressure (mmHg)",
       y="Cumulative probability of value") +
  scale_color_manual(values=c("#88398a", "gray"), name = "") 
  
```

At the widest gap between these two curves, the cumulative probability of systolic blood pressure for Male and Female are .11 apart, giving a test statistic of D = .11. The probability of getting a test statistic this large or larger is determined by examining the K-S distribution, similar to how the p-value was determined for the t-test, chi-squared, and other tests so far. In this case, the probability of .11 difference between the two was very tiny (p < .05), so the difference would be reported as statistically significant.

#### Over achiever 6: Check your understanding

Next to each non-parametric test, put the parametric test is is an alternative for. Use: one-sample t-test, independent samples t-test, and paired samples t-test.

* K-S test 
* Mann-Whitney U test 
* Wilcoxon rank sum test 
* Wilcoxon signed-rank test 

## Chapter summary

### Achievements unlocked in this chapter: Recap

After reading this chapter and following along, Leslie (and you) has learned and practiced: 

#### Achievement 1 recap: Using graphics and descriptive statistics to make a prediction

Prior to conducting inferential statistical tests like chi-squared, it is useful to get some idea of the characteristics and relationships in your data. Descriptive statistics and graphs, or exploratory data analysis (EDA), can serve two purposes: (1) understand the people, things, or phenomena you are studying better, and (2) make an educated prediction about the likely results of a statistical test, which can help identify issues if (or when) the test is not properly conducted.

#### Achievement 2 recap: One sample t-test

The t-test compares means to determine if one mean is statistically significantly different from another. There are three types of t-test: one sample, independent samples, paired samples. The one sample t-test compares the mean of one variable to a hypothesized or population mean. Significant results indicate that the difference between the means likely reflects a difference in means from the populations the samples came from.

#### Achievement 3 recap: Independent samples t-test

The independent samples t-test compares means from two unrelated groups (e.g., males and females). Significant results indicate that the difference between the means likely reflects a difference in means from the populations the samples came from.

#### Achievement 4 recap: Paired samples t-test

The paired samples t-test compares means from related groups (e.g., pre and post measures on the same person). Significant results indicate that the difference between the means likely reflects a difference in means from the populations the samples came from.

#### Achievement 5 recap: Introducing assumptions

Statistical tests rely on underlying assumptions about the characteristics of the data. When these assumptions are not met, the results may not reflect the true relationships among the variables. 

#### Achievement 6 recap: Sign test, Mann-Whitney U, and other alternative tests when assumptions are not met

When assumptions are not met for t-tests, there are several alternative tests that compare medians or distributions rather than means. 

### Chapter exercises 

The coder and hacker exercises are an opportunity to apply the skills from this chapter to a new scenario or a new data set. The coder edition will evaluate your application of the commands learned in this chapter (and earlier chapters) to similar scenarios to those in the chapter; the hacker edition will evaluate your use of the procedures from this chapter in new scenarios, usually going a step beyond what was explicitly explained. 

Before picking the coder or hacker version, check your knowledge. We recommend the coder edition if you answer all 5 multiple choice questions correctly by your third try and the hacker edition if you answer at least 3 of the 5 multiple choice questions correctly on your first try the rest correctly on your first or second try.

Q1: Which of the following tests would be used to test the mean of a continuous variable to a population mean? 

a. K-S test*  
b. Mann-Whitney U test 
c. Wilcoxon rank sum test 
d. Wilcoxon signed-rank test 

Q2: What is the primary purpose of the three t-tests?

a. Comparing means among groups* 
b. Comparing medians among groups 
c. Examining the relationship between two categorical variables 
d. Identifying normally distributed data 

Q3: Which of the following assumptions does NOT apply to all three t-tests? 

a. Independent observations 
b. Normal distribution of continuous variable  
c. Homogeneity of variances* 
d. Includes one continuous variable  

Q4: Which t-test would you use to compare mean BMI in sets of two brothers? 

a. one-sample t-test 
b. independent samples t-test 
c. chi-squared t-test 
d. paired samples t-test* 

Q5: When an independent samples t-test does not meet the assumption of normality, what is an appropriate alternative test? 

a. Sign test 
b. Levene's test 
c. Mann-Whitney U test* 
d. paired samples t-test 


#### Chapter exercises: Coder edition 

Depending on your score in the knowledge check, choose either the coder or hacker edition of the chapter exercises. Use the NHANES data from this chapter and the appropriate tests to examine diastolic blood pressure for Males and Females. 

1) Open the 2015 - 2016 NHANES data using the strategy shown in this chapter 
2) Clean the sex, diastolic blood pressure measure 1, and diastolic blood pressure measure 2 so they have clear variable names, category labels, and missing value coding 
3) (**A1**) Use graphics and descriptive statistics to examine diastolic blood pressure *measure 1* on its own and by participant sex
4) (**A1**) Use graphics and descriptive statistics to examine diastolic blood pressure *measure 2* on its own 
5) (**A1**) Based on the graphs and statistics from questions 3 and 4, make predictions about what you would find when you compare the mean DBP from measure 1 and measure 2
6) (**A1**) Based on the graphs and statistics from questions 3 and 4, make predictions about what you would find when you compare the mean DBP from measure 1 by sex
7) (**A3**, **A5**, **A6**) Select and use the appropriate t-tests to compare DBP measure 1 for Males and Females, interpret your results using the test statistics and p-value along with a graph showing the two groups. Check assumptions for this test. If the assumptions were not met, conduct and interpret the appropriate alternate test. 
8) (**A4**, **A5**, **A6**) Select and use the appropriate t-tests to compare the means of DBP measure 1 and measure 2, interpret your results using the test statistics and p-value. Check assumptions for this test. If the assumptions were not met, conduct and interpret the appropriate alternate test. 

#### Chapter exercises: Hacker edition

Complete #1 through #8 of the coder edition, then complete the following:

9) Create a new variable by recoding the `RIDAGEYR` variable into categories: 18-49, 50+. The new variable should have a logical name, clear labels, and appropriate missing value codes.
10) (**A2**, **A5**) Restrict the data to a subset of people under 50 years old. Using the appropriate test, compare their mean DBP to the normal threshold of 80. Interpret your results and check the test assumptions. If the test does not meet assumptions, conduct and interpret the appropriate alternate test. 
11) (**A2**, **A5**) Restrict the data to a subset of people 50+ years old. Using the appropriate test, compare their mean DBP to the normal threshold of 80. Interpret your results and check the test assumptions. If the test does not meet assumptions, conduct and interpret the appropriate alternate test. 

### BOXES

#### Kiara's reproducibility resource: importing NHANES data directly into R with RNHANES

<img align = "left" src = "graphics\kiara.gif" style="PADDING-RIGHT: 30px">

```{r eval = FALSE}
# import nhanes 2015-2016 BPX file with demographics
nhanes.2016 <- nhanes_load_data("BPX", "2015-2016", demographics = TRUE)

```

#### Leslie's stats stuff: what does the t-test have to do with beer?

<img align = "left" src = "graphics\leslie.gif" style="PADDING-RIGHT: 30px">

#### Nancy's fancy code: formulas in R

<img align = "left" src = "garaphics\nancy.gif" style="PADDING-RIGHT: 30px">

R uses the tilde, or `~`, to separate the right hand side and left hand side of a formula. A formula typically contains one variable that is being explained by one or more other variables. For example, income and sex may aid in explaining smoking status so a formula might be `smoking status ~ income + sex`. 

In a formula in R, the variable that is being explained is on the left hand side of the formula. The variables that are explaining are on the right hand side of the formula. The tilde that separates the right and left sides of the formula can usually be read as "is dependent on" or "is explained by." For example, in the Mann-Whitney U test explained earlier, the formula in the parentheses is systolic blood pressure explained by sex:

```{r eval = FALSE}
# compare systolic blood pressure by sex
wilcox.test(formula = nhanes.2016.cleaned$systolic ~ nhanes.2016.cleaned$sex)
```

There may be other ways of reading the `~`, but for this chapter and the next few chapters, try these two choices. Use the help documentation by typing `?tilde` at the R prompt to learn more. 


> Gamification idea for Chapter 6: Performance on student work in the achievements unlocked, are you a coder or hacker, and chapter exercises in this chapter could be included as part of earning a continuous data analysis badge. Combine scores from this chapter and chapters 10 and 11. Set thresholds for earning a bronze, silver, or gold badge. Integrate badge earnings into Blackboard or other platforms students use to share their portfolios and achievements with prospective employers or degree programs. Create a class contest for students to create a decision tree for choosing descriptive and bivariate statistics depending on variable type and assumptions. Students can work as individuals or groups. Decision trees are posted online and the class votes for the "best" decision tree. The individual or group responsible gets additional points for the chapter 6 exercises. 


